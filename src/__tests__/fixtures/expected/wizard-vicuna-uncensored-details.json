{
  "name": "wizard-vicuna-uncensored",
  "description": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.",
  "downloads": "0",
  "lastUpdated": "TIMESTAMP_PLACEHOLDER",
  "readmeHtml": "<p>Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford. The models were trained against LLaMA-7B with a subset of the dataset, responses that contained alignment / moralizing were removed.</p>\n\n<h2>Get started with Wizard Vicuna Uncensored</h2>\n\n<p>The model used in the example below is the Wizard Vicuna Uncensored model, with 7b parameters, which is a general-use model.</p>\n\n<h3>API</h3>\n\n<ol>\n<li>Start Ollama server (Run <code>ollama serve</code>)</li>\n<li>Run the model</li>\n</ol>\n\n<pre><code>curl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"wizard-vicuna-uncensored\",\n  \"prompt\":\"Who made Rose promise that she would never let go?\"\n }'\n</code></pre>\n\n<h3>CLI</h3>\n\n<ol>\n<li>Install Ollama</li>\n<li>Open the terminal and run <code>ollama run wizard-vicuna-uncensored</code></li>\n</ol>\n\n<p>Note: The <code>ollama run</code> command performs an <code>ollama pull</code> if the model is not already downloaded. To download the model without running it, use <code>ollama pull wizard-vicuna-uncensored</code></p>\n\n<h2>Memory requirements</h2>\n\n<ul>\n<li>7b models generally require at least 8GB of RAM</li>\n<li>13b models generally require at least 16GB of RAM</li>\n<li>30b models generally require at least 32GB of RAM</li>\n</ul>\n\n<p>If you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.</p>\n\n<h2>Model variants</h2>\n\n<p>By default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.</p>\n\n<table>\n<thead>\n<tr>\n<th><strong>Aliases</strong></th>\n</tr>\n</thead>\n\n<tbody>\n<tr>\n<td>latest, 7b, 7b-q4_0</td>\n</tr>\n\n<tr>\n<td>13b, 13b-q4_0</td>\n</tr>\n\n<tr>\n<td>30b, 30b-q4_0</td>\n</tr>\n</tbody>\n</table>\n\n<h2>Model source</h2>\n\n<p><strong>Wizard Vicuna Uncensored source on Ollama</strong></p>\n\n<p>7b parameters original source:\n <a href=\"https://huggingface.co/ehartford/Wizard-Vicuna-7B-Uncensored\" rel=\"nofollow\">Eric Hartford</a></p>\n\n<p>13b parameters original source:\n <a href=\"https://huggingface.co/ehartford/Wizard-Vicuna-13B-Uncensored\" rel=\"nofollow\">Eric Hartford</a></p>\n\n<p>30b parameters original source:\n <a href=\"https://huggingface.co/ehartford/Wizard-Vicuna-30B-Uncensored\" rel=\"nofollow\">Eric Hartford</a></p>",
  "readmeMarkdown": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford. The models were trained against LLaMA-7B with a subset of the dataset, responses that contained alignment / moralizing were removed.\n\n## Get started with Wizard Vicuna Uncensored\n\nThe model used in the example below is the Wizard Vicuna Uncensored model, with 7b parameters, which is a general-use model.\n\n### API\n\n1.  Start Ollama server (Run `ollama serve`)\n2.  Run the model\n\n```\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"wizard-vicuna-uncensored\",\n  \"prompt\":\"Who made Rose promise that she would never let go?\"\n }'\n```\n\n### CLI\n\n1.  Install Ollama\n2.  Open the terminal and run `ollama run wizard-vicuna-uncensored`\n\nNote: The `ollama run` command performs an `ollama pull` if the model is not already downloaded. To download the model without running it, use `ollama pull wizard-vicuna-uncensored`\n\n## Memory requirements\n\n-   7b models generally require at least 8GB of RAM\n-   13b models generally require at least 16GB of RAM\n-   30b models generally require at least 32GB of RAM\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n## Model variants\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n| **Aliases** |\n| --- |\n| latest, 7b, 7b-q4\\_0 |\n| 13b, 13b-q4\\_0 |\n| 30b, 30b-q4\\_0 |\n\n## Model source\n\n**Wizard Vicuna Uncensored source on Ollama**\n\n7b parameters original source: [Eric Hartford](https://huggingface.co/ehartford/Wizard-Vicuna-7B-Uncensored)\n\n13b parameters original source: [Eric Hartford](https://huggingface.co/ehartford/Wizard-Vicuna-13B-Uncensored)\n\n30b parameters original source: [Eric Hartford](https://huggingface.co/ehartford/Wizard-Vicuna-30B-Uncensored)",
  "models": [
    {
      "name": "latest",
      "size": "3.8GB",
      "contextWindow": "2K",
      "inputType": "Text",
      "lastUpdated": ""
    },
    {
      "name": "7b",
      "size": "3.8GB",
      "contextWindow": "2K",
      "inputType": "Text",
      "lastUpdated": ""
    },
    {
      "name": "13b",
      "size": "3.8GB",
      "contextWindow": "2K",
      "inputType": "Text",
      "lastUpdated": ""
    },
    {
      "name": "30b",
      "size": "3.8GB",
      "contextWindow": "2K",
      "inputType": "Text",
      "lastUpdated": ""
    }
  ]
}